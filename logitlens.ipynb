{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "wLBLe0a-p4ny"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/201524495/201524495/blob/main/logitlens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPNKgeFEelIr"
      },
      "source": [
        "# Logit Lens\n",
        "실습 내용:\n",
        "- **SLogit Lens**로 **intermediate hidden representation을 vocabulary logit으로 투영**해 레이어별 예측을 분석\n"
      ],
      "id": "jPNKgeFEelIr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto%2Cq_auto/v1/mirroredImages/AcKRB8wDpdaN6v6ru/ccfmt4rt3aegjjfi7lo8)  \n",
        "![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto%2Cq_auto/v1/mirroredImages/AcKRB8wDpdaN6v6ru/u4idlaozp3dnnom3qitn)  \n",
        "출처: *Interpreting GPT: the Logit Lens* (LessWrong, 2020)."
      ],
      "metadata": {
        "id": "FO2a6kKfe5Ph"
      },
      "id": "FO2a6kKfe5Ph"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg3sQbC1elIt"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# !pip install -q transformers torch\n",
        "import os, torch, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "id": "mg3sQbC1elIt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGka6m1felIu"
      },
      "source": [
        "## 1) HF 모델 load\n"
      ],
      "id": "TGka6m1felIu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRTn8AglelIu"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "MODEL_NAME = os.getenv(\"HF_MODEL_LENS\", \"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, output_hidden_states=True).to(device).eval()\n",
        "\n",
        "### ToDo : Unembedding layer (logit predictor) 구현\n",
        "W_U = None  # [V, d]\n"
      ],
      "id": "PRTn8AglelIu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoUsB6rCelIu"
      },
      "source": [
        "## 2) Logit Lens utility\n"
      ],
      "id": "IoUsB6rCelIu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RebGTPKIelIv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_logit_lens(prompt: str):\n",
        "    toks = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks)\n",
        "        hiddens = out.hidden_states\n",
        "\n",
        "    ### ToDo : Layer-wise logit lens 구현 (list of logits per layer)\n",
        "    logits_per_layer = None\n",
        "    last_ix = int(toks[\"input_ids\"].shape[1]-1)\n",
        "    return toks, logits_per_layer, last_ix\n",
        "\n",
        "def topk_tokens(vec, k=5):\n",
        "    topv, topi = torch.topk(vec, k)\n",
        "    return [tokenizer.decode([int(i)]) for i in topi], [float(v) for v in topv]\n"
      ],
      "id": "RebGTPKIelIv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82wblsskelIv"
      },
      "source": [
        "## 3) 입력 prompt에 대한 top-3 word 추출\n"
      ],
      "id": "82wblsskelIv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06p3hGV0elIw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Paris is capital of\",\n",
        "    \"Alan Turing was a\",\n",
        "    \"The most famous sports in USA is\"\n",
        "]\n",
        "for prompt in prompts:\n",
        "    toks, logits_per_layer, last_ix = run_logit_lens(prompt)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    for L, logits in enumerate(logits_per_layer):\n",
        "        tops, vals = topk_tokens(logits[0, last_ix], k=3)\n",
        "        print(f\"  Layer {L:02d}: {tops}\")\n",
        "\n"
      ],
      "id": "06p3hGV0elIw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr6xXf8HelIw"
      },
      "source": [
        "## 4) 타겟 토큰 확률의 레이어별 변화\n"
      ],
      "id": "vr6xXf8HelIw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObpqvldbelIw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "target_token = \" football\"\n",
        "prompt = \"The most famous sports in USA is\"\n",
        "toks, logits_per_layer, last_ix = run_logit_lens(prompt)\n",
        "target_id = tokenizer.encode(target_token)[0]\n",
        "probs = []\n",
        "for logits in logits_per_layer:\n",
        "    ### ToDo : logit -> probability 구현\n",
        "    p = None\n",
        "    probs.append(float(p))\n",
        "plt.figure(); plt.plot(range(len(probs)), probs, marker='o')\n",
        "plt.xlabel(\"Layer\"); plt.ylabel(f\"P({target_token.strip()})\"); plt.title(\"Layer-wise probability\"); plt.grid(True); plt.show()\n"
      ],
      "id": "ObpqvldbelIw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPAhwNrelIw"
      },
      "source": [
        "## 5) 타겟 token rank 추적\n"
      ],
      "id": "mmPAhwNrelIw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srBFvnEQelIw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def token_rank(logits, token_id):\n",
        "    return 1 + int((logits > logits[token_id]).sum())\n",
        "\n",
        "prompt = \"The most famous sports in USA is\"\n",
        "gold = \" football\"\n",
        "toks = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "gold_id = tokenizer.encode(gold)[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(**toks); hiddens = out.hidden_states\n",
        "\n",
        "logits_per_layer = [torch.matmul(h, W_U.T).cpu() for h in hiddens]\n",
        "last_ix = int(toks[\"input_ids\"].shape[1]-1)\n",
        "ranks = []\n",
        "for logits in logits_per_layer:\n",
        "    vec = logits[0, last_ix]; ranks.append(token_rank(vec, gold_id))\n",
        "\n",
        "plt.figure(); plt.plot(range(len(ranks)), ranks, marker='o')\n",
        "plt.xlabel(\"Layer\"); plt.ylabel(f\"rank({gold.strip()}) ↓ better\"); plt.title(\"Gold token rank by layer\")\n",
        "plt.gca().invert_yaxis(); plt.grid(True); plt.show()\n"
      ],
      "id": "srBFvnEQelIw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDVbM2pKelIx"
      },
      "source": [
        "## 7) Reference\n"
      ],
      "id": "cDVbM2pKelIx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f594E--telIx"
      },
      "source": [
        "- LessWrong — *Interpreting GPT: the Logit Lens* (2020): https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens  \n",
        "- TransformerLens: https://github.com/TransformerLensOrg/TransformerLens  \n",
        "- Tuned Lens: https://github.com/AlignmentResearch/tuned-lens\n"
      ],
      "id": "f594E--telIx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정답"
      ],
      "metadata": {
        "id": "wLBLe0a-p4ny"
      },
      "id": "wLBLe0a-p4ny"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = os.getenv(\"HF_MODEL_LENS\", \"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, output_hidden_states=True).to(device).eval()\n",
        "W_U = model.lm_head.weight.detach()  # [V, d]\n"
      ],
      "metadata": {
        "id": "6jTSNg3Ap57B"
      },
      "id": "6jTSNg3Ap57B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_logit_lens(prompt: str):\n",
        "    toks = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks)\n",
        "        hiddens = out.hidden_states\n",
        "\n",
        "    ### ToDo : Layer-wise logit lens 구현 (list of logits per layer)\n",
        "    logits_per_layer = None\n",
        "    last_ix = int(toks[\"input_ids\"].shape[1]-1)\n",
        "    return toks, logits_per_layer, last_ix\n",
        "\n",
        "def topk_tokens(vec, k=5):\n",
        "    topv, topi = torch.topk(vec, k)\n",
        "    return [tokenizer.decode([int(i)]) for i in topi], [float(v) for v in topv]\n"
      ],
      "metadata": {
        "id": "WHblD-JLp7tr"
      },
      "id": "WHblD-JLp7tr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_token = \" football\"\n",
        "prompt = \"The most famous sports in USA is\"\n",
        "toks, logits_per_layer, last_ix = run_logit_lens(prompt)\n",
        "target_id = tokenizer.encode(target_token)[0]\n",
        "probs = []\n",
        "for logits in logits_per_layer:\n",
        "    ### ToDo : logit -> probability 구현\n",
        "    p = None\n",
        "    probs.append(float(p))\n",
        "plt.figure(); plt.plot(range(len(probs)), probs, marker='o')\n",
        "plt.xlabel(\"Layer\"); plt.ylabel(f\"P({target_token.strip()})\"); plt.title(\"Layer-wise probability\"); plt.grid(True); plt.show()\n"
      ],
      "metadata": {
        "id": "daMpuInNqCSh"
      },
      "id": "daMpuInNqCSh",
      "execution_count": null,
      "outputs": []
    }
  ]
}