{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/201524495/201524495/blob/main/250812%EC%83%9D%EC%84%B1%EB%AA%A8%EB%8D%B8%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0h80hnjWfls"
      },
      "source": [
        "# 실습 1: Text & Image 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvG_Qcd1rCKf",
        "outputId": "8b2fbaf2-755d-454b-a63a-c9c9abf76ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvxXrjGUrMa8",
        "outputId": "f7cf1995-04e1-4ec1-fbda-427c28785d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-vgafq8o7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-vgafq8o7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=2e9798c49cc8c91d26e4c2f046178a517415d73d59369c8c13eb842ee23727c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y9g275eu/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers diffusers\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCpxjj3MJ4fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from diffusers import StableDiffusionPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETf8gc92V-tm"
      },
      "source": [
        "### 필요한 모델을 먼저 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxf_JuMhV-Rg"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 텍스트 생성 모델\n",
        "# generator = pipeline(\"text-generation\", model=\"openai/gpt-oss-20b\")\n",
        "generator = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "\n",
        "# 이미지 생성 모델\n",
        "model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEhAqF4qMRjI"
      },
      "source": [
        "## Stable Diffusion (확산모델) 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRWcUYGSXA2p"
      },
      "source": [
        "텍스트 → 이미지\n",
        "\n",
        "    예: “푸른 하늘 아래 해바라기 밭”이라고 쓰면 그 장면을 그림으로 만들어줍니다.\n",
        "\n",
        "오픈소스\n",
        "\n",
        "    무료로 배포되어, 누구나 PC에 설치해 사용하거나 수정할 수 있습니다.\n",
        "\n",
        "작동 원리\n",
        "\n",
        "    처음엔 ‘노이즈’의 이미지에서 시작 → 글(프롬프트)을 보고 조금씩 노이즈를 지워 → 최종 그림 완성.\n",
        "\n",
        "    이 과정을 diffusion(확산) 모델이라고 부릅니다.\n",
        "\n",
        "필수 용어\n",
        "\n",
        "    프롬프트: 원하는 그림을 설명하는 문장\n",
        "\n",
        "    시드(seed): 같은 설정으로 재현 가능한 ‘랜덤 값’\n",
        "\n",
        "    샘플러: 노이즈를 지우는 방식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngQHaL6TrVDm"
      },
      "outputs": [],
      "source": [
        "# model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
        "model.to(\"cuda\")  # GPU 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WUbxty_soP6"
      },
      "outputs": [],
      "source": [
        "# Sampling Steps: 이미지 품질을 조정하는 파라미터인 num_inference_steps 설정.\n",
        "\n",
        "prompt = \"A beautiful sunset over the ocean with a calm sky\"\n",
        "result = model(prompt, num_inference_steps=20).images[0]\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyNIk_hJL1mj"
      },
      "outputs": [],
      "source": [
        "prompt = \"A surrealist painting of a lion made of clouds, with a dreamy atmosphere and soft pastel colors\"\n",
        "result = model(prompt, num_inference_steps=10).images[0]\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8q8mvHEtU2x"
      },
      "outputs": [],
      "source": [
        "prompt = \"A surrealist painting of a lion made of clouds, with a dreamy atmosphere and soft pastel colors\"\n",
        "result = model(prompt, num_inference_steps=50).images[0]\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJaq8gN0tWHU"
      },
      "outputs": [],
      "source": [
        "prompt = \"A surrealist painting of a lion made of clouds, with a dreamy atmosphere and soft pastel colors\"\n",
        "result = model(prompt, num_inference_steps=100).images[0]\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci-dI-SLuioX"
      },
      "outputs": [],
      "source": [
        "prompt = \"A surrealist painting of a lion made of clouds, with a dreamy atmosphere and soft pastel colors\"\n",
        "result = model(prompt, num_inference_steps=200).images[0]\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmZhHtVxVlma"
      },
      "source": [
        "### Inferece Steps의 의미"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmcREzWTVlBK"
      },
      "outputs": [],
      "source": [
        "prompt = \"A surrealist painting of a lion made of clouds, with a dreamy atmosphere and soft pastel colors\"\n",
        "for num_step in [1, 10, 50, 75, 100]:\n",
        "  result = model(prompt, num_inference_steps=num_step).images[0]\n",
        "  display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGUi2YKK1QA"
      },
      "source": [
        "## Prompt diversification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-TPvYuTG_Zy"
      },
      "outputs": [],
      "source": [
        "# First prompt for generating a concept of 'dog'\n",
        "cls_tmp = \"a photo of dog\"\n",
        "\n",
        "first_stage_prompt = '''\n",
        "To generate images using a text-to-image generation model, I need to create a prompt for a specific concept.\n",
        "Your concept is {cls_tmp}. Keep the domain photorealistic, and use diverse visual scenes, visual styles, or color palettes.\n",
        "\n",
        "Please create one prompt sentence (under 10 words) that fits this description.\n",
        "Please ensure the response format is strictly 'prompt: answer' and include the phrase {cls_tmp}.\n",
        "'''\n",
        "\n",
        "# Following prompt for generating a concept of 'dog'\n",
        "second_stage_prompt = '''\n",
        "To generate images using a text-to-image generation model, I need to create a prompt for a specific concept.\n",
        "Your concept is {cls_tmp}. Keep the domain photorealistic, and use diverse visual scenes, visual styles, or color palettes.\n",
        "\n",
        "Here is a list of prompts that I have previously generated.\n",
        "Please create a new prompt that does not overlap with these.\n",
        "\n",
        "{first_stage_results}\n",
        "\n",
        "Please create one prompt sentence (under 10 words) that fits this description.\n",
        "Please ensure the response format is strictly 'prompt: answer' and include the phrase {cls_tmp}.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2895af53"
      },
      "source": [
        " `generator` object로 텍스트를 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc903969"
      },
      "outputs": [],
      "source": [
        "def get_response(prompt):\n",
        "  # Generate text based on a prompt\n",
        "  generated_text = generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "\n",
        "  return generated_text[0]['generated_text'][len(prompt):].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iDzi3HbTiKZ"
      },
      "outputs": [],
      "source": [
        "get_response(\"Hello, I'm a language model,\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j63WneLCt-Eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "get_response(first_stage_prompt.format(cls_tmp=cls_tmp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Muv8egS9WW"
      },
      "outputs": [],
      "source": [
        "first_stage_results = []\n",
        "prompt = first_stage_prompt.format(cls_tmp=cls_tmp)\n",
        "for _ in range(2):\n",
        "  response = get_response(prompt)\n",
        "  print(response)\n",
        "  generated_image = model(response.replace(\"prompt:\", \"\").strip(), num_inference_steps=20).images[0]\n",
        "  display(generated_image)\n",
        "\n",
        "  first_stage_results.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIy25rF5UaaF"
      },
      "outputs": [],
      "source": [
        "prompt = second_stage_prompt.format(cls_tmp=cls_tmp, first_stage_results=first_stage_results)\n",
        "response = get_response(prompt)\n",
        "print(response)\n",
        "generated_image = model(response, num_inference_steps=20).images[0]\n",
        "display(generated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwllFBRoWKNV"
      },
      "source": [
        "### 오픈 모델 사용하기 - from HuggingFace.\n",
        "https://huggingface.co/models?pipeline_tag=text-generation&sort=trending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRAzevI_XuN3"
      },
      "source": [
        "### Heuristics:  프롬프트 팁\n",
        "\n",
        "[주제] + [상세 묘사] + [스타일/분위기] + [구도/시점] + [기술적 태그]\n",
        "\n",
        "    [주제]: 무엇을 그릴지 (인물, 동물, 풍경, 사물 등)\n",
        "\n",
        "    [상세 묘사]: 나이, 표정, 옷, 색상, 배경 요소\n",
        "\n",
        "    [스타일/분위기]: realistic, anime, watercolor, cyberpunk, fantasy 등\n",
        "\n",
        "    [구도/시점]: close-up, full body, aerial view, portrait 등\n",
        "\n",
        "    [기술적 태그]: high detail, ultra realistic, 8k, cinematic lighting 등\n",
        "\n",
        "\n",
        "\"A cute cat wearing a wizard hat, holding a magic staff, fantasy forest background, watercolor style, high detail, soft lighting\"\n",
        "\n",
        "\"Surreal dreamscape, floating islands, waterfalls falling into the sky, pastel colors, fantasy style, highly detailed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nALR42B5X_cf"
      },
      "outputs": [],
      "source": [
        "generated_image = model(\"A cute cat wearing a wizard hat, holding a magic staff, fantasy forest background, watercolor style, high detail, soft lighting\"\n",
        ", num_inference_steps=50).images[0]\n",
        "generated_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT7VC59dKrDF"
      },
      "source": [
        "## CLIP (시각-언어모델)을 활용해서 원치않는 결과 필터링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqsZz2uFJ_sd"
      },
      "outputs": [],
      "source": [
        "# CLIP 모델 로드\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCIM09_ZKWb3"
      },
      "outputs": [],
      "source": [
        "# 이미지 전처리 함수\n",
        "def preprocess_image(image):\n",
        "    \"\"\"Colab 변수 result를 CLIP 입력에 맞게 전처리\"\"\"\n",
        "    return preprocess(image).unsqueeze(0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPhxhwmGKpwm"
      },
      "outputs": [],
      "source": [
        "# 텍스트 리스트 준비\n",
        "texts = [\"a dog\", \"a cat\", \"a person running\", \"a beautiful landscape\", \"a lion\"]\n",
        "text_tokens = clip.tokenize(texts).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKx7cHLHKWWg"
      },
      "outputs": [],
      "source": [
        "# Colab 변수 result를 사용\n",
        "display(result)\n",
        "image_tensor = preprocess_image(result)\n",
        "\n",
        "# CLIP 인코딩\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_tensor)\n",
        "    text_features = clip_model.encode_text(text_tokens)\n",
        "\n",
        "# 유사도 계산 (코사인 유사도)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "# 결과 출력\n",
        "for i, (text, score) in enumerate(zip(texts, similarities.squeeze(0).tolist())):\n",
        "    print(f\"{text}: {score:.4f}\") # score가 1에 가까울수록 text-image가 잘 어울리는 쌍이라는 뜻입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_bIU_wBWZlr"
      },
      "source": [
        "# 실습 2: 멀티모달 생성 결과를 평가하는 지표들"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ4CgfdDj9ap"
      },
      "source": [
        "* 정성적 평가(Qualitative): 사람 평가(리커트 척도), 쌍 비교 선호도, 전문가 리뷰, 루브릭 기반 점수.\n",
        "* 정량적 평가(Quantitative, 자동):\n",
        "\n",
        "    * 텍스트: n-그램 중첩(BLEU/ROUGE), 임베딩 유사도(BERTScore)\n",
        "\n",
        "    * 이미지: 분포 간 거리(FID), 다양성+품질 (IS), 텍스트-이미지 유사도(CLIPScore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSeIc38EkQqj"
      },
      "source": [
        "## 텍스트 평가 지표: BLEU, ROUGE, BERTScore\n",
        "\n",
        "아래 지표들은 예측(predictions)과 정답(references)을 비교합니다.\n",
        "\n",
        "* BLEU: precision 중심의 n-그램 중첩(기계번역에서 시작)\n",
        "* ROUGE: recall 중심(텍스트 요약에서 시작)  https://aclanthology.org/W04-1013.pdf\n",
        "\n",
        "* BERTScore: 문맥 임베딩을 활용한 의미 유사도 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAGWZHQ2klu4"
      },
      "outputs": [],
      "source": [
        "!pip -q install evaluate rouge_score bert-score clean-fid torch-fidelity ftfy regex tqdm git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yUnW4_a1cXj"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import evaluate  # https://huggingface.co/docs/evaluate/en/index\n",
        "\n",
        "# Sample predictions & references\n",
        "predictions = [\n",
        "    \"The cat is on the mat.\",\n",
        "    \"A young woman in a red kimono stands under cherry blossoms.\"\n",
        "]\n",
        "references = [\n",
        "    [\"The cat sits on the mat.\"],\n",
        "    [\"A girl wearing a red kimono is standing beneath cherry blossoms.\"]\n",
        "]\n",
        "\n",
        "# BLEU\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_res = bleu.compute(predictions=predictions, references=references)\n",
        "print(\"BLEU\")\n",
        "pprint(bleu_res)\n",
        "\n",
        "# ROUGE (expects flat reference strings, not nested)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_res = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
        "print(\"\\nROUGE\")\n",
        "pprint(rouge_res)\n",
        "\n",
        "# BERTScore\n",
        "from bert_score import score\n",
        "P, R, F1 = score(predictions, [r[0] for r in references], lang=\"en\", verbose=True)\n",
        "print(\"\\nBERTScore\")\n",
        "print(\"Precision:\", float(P.mean()))\n",
        "print(\"Recall   :\", float(R.mean()))\n",
        "print(\"F1       :\", float(F1.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiEUZreT7ZrB"
      },
      "source": [
        "[Why BertScore?](https://colab.research.google.com/drive/1fs90QoAP_Z78Cb2B5fTfvvdT8FWPGpL6?usp=sharing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV2gnbSAntst"
      },
      "source": [
        "## 이미지 평가 지표: FID, IS, CLIPScore\n",
        "\n",
        "진행 순서:\n",
        "* 실제(real) 고양이 이미지 소규모 세트를 다운로드합니다.\n",
        "* 실제 이미지를 변형(블러/노이즈 등)해 품질이 낮은 “합성(synthetic)” 세트를 만듭니다.\n",
        "* 두 세트 간 **FID**와 **IS**를 계산합니다.\n",
        "* 한 장의 이미지와 텍스트 프롬프트 간 **CLIPScore**를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAYSlHFnki1F"
      },
      "outputs": [],
      "source": [
        "import os, shutil, urllib.request, pathlib, random\n",
        "from PIL import Image, ImageFilter\n",
        "from io import BytesIO\n",
        "\n",
        "base = pathlib.Path(\"multimodal_eval_data\")\n",
        "real_dir = base / \"real\"\n",
        "fake_dir = base / \"fake\"\n",
        "for d in [real_dir, fake_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Small sample of CC-friendly cat photos (Unsplash CDN)\n",
        "urls = [\n",
        "    \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\",\n",
        "    \"https://images.unsplash.com/photo-1511044568932-338cba0ad803\",\n",
        "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba\",\n",
        "    \"https://images.unsplash.com/photo-1495360010541-f48722b34f7d\",\n",
        "    \"https://images.unsplash.com/photo-1555685812-4b943f1cb0eb\",\n",
        "]\n",
        "\n",
        "def dl(url, out_path, size=(512, 512)):\n",
        "    # Add parameters for reasonable size and resize to a fixed size\n",
        "    full = url + \"?auto=format&fit=crop&w=512&q=80\"\n",
        "    with urllib.request.urlopen(full) as r:\n",
        "        img = Image.open(BytesIO(r.read())).convert(\"RGB\")\n",
        "        # Resize without preserving aspect ratio to ensure uniform size\n",
        "        img = img.resize(size)\n",
        "        img.save(out_path)\n",
        "\n",
        "# Download real images\n",
        "for i, u in enumerate(urls):\n",
        "    dl(u, real_dir / f\"real_{i:02d}.jpg\")\n",
        "\n",
        "# Create \"fake\" images by degrading (blur + down/up sampling + noise-like effect)\n",
        "for p in real_dir.glob(\"*.jpg\"):\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    small = img.resize((128, 128))\n",
        "    blurred = small.filter(ImageFilter.GaussianBlur(2))\n",
        "    # Resize without preserving aspect ratio to ensure uniform size\n",
        "    degraded = blurred.resize((512, 512))\n",
        "    degraded.save(fake_dir / p.name)\n",
        "\n",
        "len(list(real_dir.glob(\"*.jpg\"))), len(list(fake_dir.glob(\"*.jpg\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWH0IPbKoAbM"
      },
      "outputs": [],
      "source": [
        "import torch_fidelity\n",
        "\n",
        "metrics = torch_fidelity.calculate_metrics(\n",
        "    input1=str(real_dir),\n",
        "    input2=str(fake_dir),\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=True,              # Inception Score\n",
        "    fid=True,              # Frechet Inception Distance\n",
        "    kid=False,\n",
        "    verbose=True,\n",
        "    isc_splits=5\n",
        ")\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n3v_qQVoBW_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, clip\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Use one of the real images\n",
        "sample_img_path = sorted(real_dir.glob(\"*.jpg\"))[0]\n",
        "image = preprocess(Image.open(sample_img_path)).unsqueeze(0).to(device)\n",
        "display(Image.open(sample_img_path))\n",
        "\n",
        "# Prompts to compare\n",
        "texts = [\n",
        "    \"A cute cat lying on a bed\",\n",
        "    \"A dog running in the park\",\n",
        "    \"A bowl of pasta on a table\"\n",
        "]\n",
        "text_tokens = clip.tokenize(texts).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_feat = model.encode_image(image)\n",
        "    txt_feat = model.encode_text(text_tokens)\n",
        "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
        "    sims = (img_feat @ txt_feat.T).squeeze(0).tolist()\n",
        "\n",
        "for t, s in zip(texts, sims):\n",
        "    print(f\"CLIP similarity: {s:.4f}  |  '{t}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB0lN2n1scIl"
      },
      "source": [
        "결과 해석\n",
        "- FID: 낮을수록 실제 이미지 분포와 더 가깝습니다.\n",
        "- IS: 높을수록 이미지가 품질이 좋고 동시에 다양성이 높다는 의미입니다.\n",
        "\n",
        "- CLIPScore(코사인 유사도): 높을수록 텍스트-이미지 의미 유사도(관련성)이 더 높습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5E-ysuspWAs"
      },
      "source": [
        "## 실습 문제\n",
        "* 샘플 이미지를 여러분이 생성한 이미지(예: Stable Diffusion 결과물)로 교체하고 FID/IS를 다시 계산해 보세요.\n",
        "* 텍스트 프롬프트를 변경한 뒤 CLIPScore를 다시 계산해 보세요 — 어떤 프롬프트가 각 이미지와 가장 잘 맞는지 확인합니다.\n",
        "* 블러/노이즈는 CLIPScore, FID, IS 중 어떤 지표에 가장 큰 영향을 줄까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfbmf7q9tDnY"
      },
      "source": [
        "블러/노이즈는 CLIPScore, FID, IS 중 어떤 지표에 가장 큰 영향을 줄까요?\n",
        "\n",
        "-> FID에 가장 큰 영향을 주는 경우가 많고, 그 다음이 IS, CLIPScore는 상대적으로 덜 민감한 편입니다.\n",
        "\n",
        "\n",
        "FID (Fréchet Inception Distance)\n",
        "\n",
        "    이미지의 전체 분포를 비교하므로, 블러·노이즈처럼 이미지 품질을 크게 해치는 변형에 민감합니다.\n",
        "\n",
        "    특히, 픽셀 레벨의 구조와 세부 질감이 손상되면 Inception feature 분포가 많이 달라져서 값이 급격히 나빠집니다.\n",
        "\n",
        "IS (Inception Score)\n",
        "\n",
        "    이미지의 다양성과 품질을 모두 반영합니다.\n",
        "\n",
        "    블러/노이즈로 인해 Inception 모델이 클래스 예측을 덜 확신하게 되면 점수가 떨어집니다.\n",
        "\n",
        "    하지만 다양성이 유지되면 하락 폭은 FID보다 작을 수 있습니다.\n",
        "\n",
        "CLIPScore\n",
        "\n",
        "    텍스트-이미지 의미 일치를 보는 지표입니다.\n",
        "\n",
        "    블러나 노이즈가 있어도, 큰 윤곽과 주요 패턴이 유지되면 CLIP은 여전히 “고양이”나 “강아지”처럼 의미를 잘 매칭합니다.\n",
        "\n",
        "    그래서 품질 저하보다는 주제 변경이나 오브젝트 손실에 더 크게 반응합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7lCDn8uo_TK"
      },
      "source": [
        "## References\n",
        "* Hugging Face evaluate (BLEU/ROUGE)\n",
        "* BERTScore (Zhang et al. 2020)\n",
        "* FID (Heusel et al. 2017), IS (Salimans * et al. 2016)\n",
        "* CLIP (Radford et al. 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqVvAn8ipCvD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}